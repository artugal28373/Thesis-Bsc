# -*- coding: utf-8 -*-
"""Copy of GDD_ML_HOG_SURF

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LODoMtOZ1d0Rxoh-Mw2yilfDTgKzKEsv
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import hog
from sklearn.model_selection import train_test_split, learning_curve, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import StratifiedShuffleSplit

dataset_path = '/content/drive/MyDrive/200_data(Guava)'

IMAGE_SIZE = 64

import cv2
import numpy as np
from skimage.feature import local_binary_pattern
from skimage.color import rgb2gray
from skimage.feature import hog

def norm(a):
    if max(a.flatten()) ==0 :
      return a
    return a / max(a.flatten())

def extract_features(img):
    # Define LBP parameters
    radius = 3
    n_points = 8 * radius

    # Resize image to a fixed size (assuming IMAGE_SIZE is defined)
    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))

    # Convert image to grayscale
    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    #print(gray_img.dtpye)

    # Compute LBP
    lbp = local_binary_pattern(gray_img, n_points, radius, method='uniform')

    # Convert image to RGB (assuming it's in BGR format)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Compute histogram for each color channel
    hist_r, _ = np.histogram(img[:,:,0], bins=256, range=(0, 256))
    hist_g, _ = np.histogram(img[:,:,1], bins=256, range=(0, 256))
    hist_b, _ = np.histogram(img[:,:,2], bins=256, range=(0, 256))



    # Compute HOG features for each color channel
    hog_features_r = (hog(img[:,:,0], pixels_per_cell=(4, 4), cells_per_block=(4, 4)))
    hog_features_g = (hog(img[:,:,1], pixels_per_cell=(4, 4), cells_per_block=(4, 4)))
    hog_features_b = (hog(img[:,:,2], pixels_per_cell=(4, 4), cells_per_block=(4, 4)))



    # Compute SIFT descriptors
    sift = cv2.xfeatures2d.SIFT_create()
    kp, sift_features = sift.detectAndCompute(gray_img, None)

    # Concatenate LBP, HOG, RGB color histogram, and SIFT features
    lbp_features = lbp.flatten()
    rgb_hist_features = np.concatenate((hist_r, hist_g, hist_b))
    hog_features = np.concatenate((hog_features_r, hog_features_g, hog_features_b))

    # If there are SIFT features, concatenate them; otherwise, create an empty array
    if sift_features is not None:
        combined_features = np.concatenate((lbp_features, rgb_hist_features, hog_features, sift_features.flatten()))
    else:
        combined_features = np.concatenate((lbp_features, rgb_hist_features, hog_features))
    return combined_features

def load_data_and_extract_features(dataset_path):
    data = []
    labels = []
    for category in os.listdir(dataset_path):
        category_path = os.path.join(dataset_path, category)
        for img_name in os.listdir(category_path):
            img_path = os.path.join(category_path, img_name)
            img = cv2.imread(img_path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            # Extract features using extract_features function
            features = extract_features(img)
            data.append(features)
            labels.append(category)
        print("Processed category:", category)
    max_length = max(len(features) for features in data)
    for i in range(len(data)):
        if len(data[i]) < max_length:
            data[i] = np.pad(data[i], (0, max_length - len(data[i])), mode='constant')
    return np.array(data), np.array(labels)

X, y = load_data_and_extract_features(dataset_path)

X = norm(X)
print(max(X.flatten()))

scaler = StandardScaler()

le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)


# Assuming X and y_encoded are already defined
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in sss.split(X, y_encoded):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y_encoded[train_index], y_encoded[test_index]

# Verify the distribution
print("Train class distribution:", np.bincount(y_train))
print("Test class distribution:", np.bincount(y_test))

X_train = scaler.fit_transform(X_train)
X_test =  scaler.fit_transform(X_test)

print(X.shape)

svm_model = SVC(kernel='linear', C=0.0001, random_state=42)
svm_model.fit(X_train, y_train)
# Make predictions
y_pred = svm_model.predict(X_test)
# Dont use PCA

import seaborn as sns
categories = list(os.listdir(dataset_path))
# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=categories))

print("Accuracy:", accuracy_score(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()