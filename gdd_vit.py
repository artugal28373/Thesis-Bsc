# -*- coding: utf-8 -*-
"""GDD_VIT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BsyYgyXVDR7Ui_WiQSjg8RWNGxQQe-Wu
"""

from google.colab import drive
drive.mount('/content/drive')

pip install torch torchvision transformers scikit-learn matplotlib

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from transformers import ViTForImageClassification, ViTFeatureExtractor
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import PIL.Image

# Custom Dataset
class CustomImageDataset(Dataset):
    def __init__(self, img_dir, transform=None):
        self.img_dir = img_dir
        self.transform = transform
        self.img_labels = []
        self.img_paths = []
        for class_name in os.listdir(img_dir):
            class_dir = os.path.join(img_dir, class_name)
            if os.path.isdir(class_dir):
                for img_name in os.listdir(class_dir):
                    self.img_labels.append(class_name)
                    self.img_paths.append(os.path.join(class_dir, img_name))
        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(sorted(set(self.img_labels)))}

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        image = PIL.Image.open(img_path).convert("RGB")
        label = self.class_to_idx[self.img_labels[idx]]
        if self.transform:
            image = self.transform(image)
        return image, label

# Set up paths
train_dir = '/content/drive/MyDrive/traing_test/train'
val_dir = '/content/drive/MyDrive/traing_test/validation'
test_dir = '/content/drive/MyDrive/traing_test/test'

# Transforms
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
])

# Load Data
train_dataset = CustomImageDataset(train_dir, transform=transform)
val_dataset = CustomImageDataset(val_dir, transform=transform)
test_dataset = CustomImageDataset(test_dir, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import ViTForImageClassification
import matplotlib.pyplot as plt

# Model
model = ViTForImageClassification.from_pretrained(
    'google/vit-base-patch16-224',
    num_labels=8,
    ignore_mismatched_sizes=True
)

# Re-initialize the classification head for 8 classes
model.classifier = nn.Linear(model.config.hidden_size, 8)

# Print to verify the classifier structure
print(model.classifier)

# Freeze the transformer layers initially
for param in model.vit.parameters():
    param.requires_grad = False

# Training setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
criterion = nn.CrossEntropyLoss()

# Separate parameters for different learning rates
optimizer = optim.AdamW([
    {'params': model.vit.parameters(), 'lr': 1e-5},
    {'params': model.classifier.parameters(), 'lr': 1e-4}
])

# Training and Validation Loop
num_epochs = 20
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images).logits
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

        _, predicted = torch.max(outputs, 1)
        correct_train += (predicted == labels).sum().item()
        total_train += labels.size(0)

    train_losses.append(running_loss / len(train_loader))
    train_accuracies.append(correct_train / total_train)

    model.eval()
    val_loss = 0.0
    correct_val = 0
    total_val = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images).logits
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_val += (predicted == labels).sum().item()
            total_val += labels.size(0)

    val_losses.append(val_loss / len(val_loader))
    val_accuracies.append(correct_val / total_val)
    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}, Train Acc: {train_accuracies[-1]}, Val Acc: {val_accuracies[-1]}")

    # Unfreeze some transformer layers gradually
    if epoch == int(num_epochs / 2):
        for param in model.vit.parameters():
            param.requires_grad = True

os.makedirs('/content/drive/MyDrive/Thesis', exist_ok=True)


torch.save(model.state_dict(), '/content/drive/MyDrive/Thesis/final_model.pth')

import pickle
os.makedirs('/content/drive/MyDrive/Thesis', exist_ok=True)
def save_training_results(train_losses, val_losses, train_accuracies, val_accuracies, filename="/content/drive/MyDrive/Thesis/vit_training_results.pkl"):
  """Saves training results to a pickle file.

  Args:
      train_losses (list): List of training losses for each epoch.
      val_losses (list): List of validation losses for each epoch.
      train_accuracies (list): List of training accuracies for each epoch (optional).
      val_accuracies (list): List of validation accuracies for each epoch (optional).
      filename (str, optional): Name of the file to save the results to. Defaults to "training_results.pkl".
  """

  with open(filename, "wb") as f:
    pickle.dump({
        "train_losses": train_losses,
        "val_losses": val_losses,
        "train_accuracies": train_accuracies if train_accuracies else [],
        "val_accuracies": val_accuracies if val_accuracies else []
    }, f)

  print(f"Training results saved to: {filename}")

# Example usage (assuming you have already populated the lists)
save_training_results(train_losses, val_losses, train_accuracies, val_accuracies)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')
plt.plot(range(1, num_epochs+1), val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss over Epochs')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(range(1, num_epochs+1), train_accuracies, label='Train Accuracy')
plt.plot(range(1, num_epochs+1), val_accuracies, label='Val Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy over Epochs')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluation
model.eval()
y_true = []
y_pred = []
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images).logits
        _, preds = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

# Calculate test accuracy
accuracy = (np.array(y_pred) == np.array(y_true)).mean()
print(f'Test Accuracy: {accuracy:.4f}')

# Classification Report
print(classification_report(y_true, y_pred, target_names=train_dataset.class_to_idx.keys()))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(train_dataset.class_to_idx.keys()))
fig, ax = plt.subplots(figsize=(10, 10))  # Adjust the figure size as needed
disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation=45)
plt.show()
import seaborn as sns
conf_matrix = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
categories = list(train_dataset.class_to_idx.keys())
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

